{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subjects(filenames):\n",
    "    subjects = []\n",
    "    for filename in filenames:\n",
    "        subject = filename.split('_')[0]\n",
    "        if subject not in subjects:\n",
    "            subjects.append(subject)\n",
    "    return subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subjects = get_subjects(os.listdir('brain_ct/train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slices(subjects):\n",
    "    slices = {}\n",
    "    for subject in subjects:\n",
    "        slices[subject] = [f'brain_ct/train/{subject}_00{i}.png' for i in range(5)]\n",
    "    return slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = get_slices(train_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Tensor of shape (5, 256, 256, 1) to store the slices per subject\n",
    "import numpy as np\n",
    "import torch \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_slices_per_subject(slices):\n",
    "    X = np.zeros((5, 256, 256, 1))\n",
    "    for i, slice in enumerate(slices):\n",
    "        X[i] = np.expand_dims(plt.imread(slice), axis=-1)\n",
    "    return X\n",
    "\n",
    "X = load_slices_per_subject(slices['1BA253'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n",
      "(5, 256, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "for key, value in slices.items():\n",
    "    volume = load_slices_per_subject(value)\n",
    "    print(volume.shape)\n",
    "    np.save(f'brain_ct/train_volume/{key}.npy', volume)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 256, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "# make a tensor of shape (256, 256, 256, 1)\n",
    "numpy_tensor = np.random.rand(256, 256, 256, 1)\n",
    "tensor = torch.from_numpy(numpy_tensor)\n",
    "\n",
    "tensor = tensor.permute(3, 0, 1, 2)\n",
    "\n",
    "tensor = tensor.unsqueeze(0)\n",
    "print(tensor.shape)\n",
    "tensor = tensor.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from daft.networks.vol_networks import DAFT \n",
    "model = DAFT(1,1)\n",
    "output = model(tensor, torch.tensor([0, 0, 0, 0, 0, 0], dtype=torch.float).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n",
      "(6,)\n"
     ]
    }
   ],
   "source": [
    "# Create random tabular data\n",
    "for i in range(len(train_subjects)):\n",
    "    tabular_data = np.random.rand( 6)\n",
    "    print(tabular_data.shape)\n",
    "    np.save(f'brain_ct/train_tabular/{train_subjects[i]}.npy', tabular_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "# Create random tabular data\n",
    "for i in range(len(train_subjects)):\n",
    "    target = np.random.rand(1)\n",
    "    print(target.shape)\n",
    "    np.save(f'brain_ct/train_target/{train_subjects[i]}.npy', target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Dataloaders \n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "class BrainCtDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.subjects = [s.split('.')[0] for s in os.listdir(root_dir)]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.subjects)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        subject = self.subjects[idx]\n",
    "        volume = np.load(f'{self.root_dir}/{subject}.npy')\n",
    "        volume = torch.from_numpy(volume).permute(3, 0, 1, 2)\n",
    "        volume = volume.float()\n",
    "        tabular = np.load(f'brain_ct/train_tabular/{subject}.npy')\n",
    "        tabular = torch.from_numpy(tabular).float()\n",
    "        target = np.load(f'brain_ct/train_target/{subject}.npy')\n",
    "        target = torch.from_numpy(target).float()\n",
    "        return volume, tabular, target\n",
    "    \n",
    "train_dataset = BrainCtDataset('brain_ct/train_volume')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 5, 256, 256])\n",
      "torch.Size([16, 6])\n",
      "torch.Size([16, 1])\n"
     ]
    }
   ],
   "source": [
    "for volumes, tabular, target in train_loader:\n",
    "    batch = volumes, tabular, target \n",
    "    print(volumes.shape)\n",
    "    print(tabular.shape)\n",
    "    print(target.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from daft.networks.vol_networks import DAFT \n",
    "model = DAFT(1,1)\n",
    "output = model(volumes, tabular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1])\n"
     ]
    }
   ],
   "source": [
    "print(output['logits'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "\n",
    "def safe_normalize(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Normalize risk scores to avoid exp underflowing.\n",
    "\n",
    "    Note that only risk scores relative to each other matter.\n",
    "    If minimum risk score is negative, we shift scores so minimum\n",
    "    is at zero.\n",
    "    \"\"\"\n",
    "    x_min, _ = torch.min(x, dim=0)\n",
    "    c = torch.zeros(x_min.shape, device=x.device)\n",
    "    norm = torch.where(x_min < 0, -x_min, c)\n",
    "    return x + norm\n",
    "\n",
    "\n",
    "def logsumexp_masked(\n",
    "    risk_scores: torch.Tensor, mask: torch.Tensor, dim: int = 0, keepdim: Optional[bool] = None\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Compute logsumexp across `dim` for entries where `mask` is true.\"\"\"\n",
    "    assert risk_scores.dim() == mask.dim(), \"risk_scores and mask must have same rank\"\n",
    "\n",
    "    mask_f = mask.type_as(risk_scores)\n",
    "    risk_scores_masked = risk_scores * mask_f\n",
    "    # for numerical stability, substract the maximum value\n",
    "    # before taking the exponential\n",
    "    amax, _ = torch.max(risk_scores_masked, dim=dim, keepdim=True)\n",
    "    risk_scores_shift = risk_scores_masked - amax\n",
    "\n",
    "    exp_masked = risk_scores_shift.exp() * mask_f\n",
    "    exp_sum = exp_masked.sum(dim, keepdim=True)\n",
    "    output = exp_sum.log() + amax\n",
    "    if not keepdim:\n",
    "        output.squeeze_(dim=dim)\n",
    "    return output\n",
    "\n",
    "class CoxphLoss(nn.Module):\n",
    "    def forward(self, predictions: torch.Tensor, event: torch.Tensor, riskset: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Negative partial log-likelihood of Cox's proportional\n",
    "        hazards model.\n",
    "\n",
    "        Args:\n",
    "            predictions (torch.Tensor):\n",
    "                The predicted outputs. Must be a rank 2 tensor.\n",
    "            event (torch.Tensor):\n",
    "                Binary vector where 1 indicates an event 0 censoring.\n",
    "            riskset (torch.Tensor):\n",
    "                Boolean matrix where the `i`-th row denotes the\n",
    "                risk set of the `i`-th instance, i.e. the indices `j`\n",
    "                for which the observer time `y_j >= y_i`.\n",
    "\n",
    "        Returns:\n",
    "            loss (torch.Tensor):\n",
    "                Scalar loss.\n",
    "\n",
    "        References:\n",
    "            .. [1] Faraggi, D., & Simon, R. (1995).\n",
    "            A neural network model for survival data. Statistics in Medicine,\n",
    "            14(1), 73–82. https://doi.org/10.1002/sim.4780140108\n",
    "        \"\"\"\n",
    "        if predictions is None or predictions.dim() != 2:\n",
    "            raise ValueError(\"predictions must be a 2D tensor.\")\n",
    "        if predictions.size()[1] != 1:\n",
    "            raise ValueError(\"last dimension of predictions ({}) must be 1.\".format(predictions.size()[1]))\n",
    "        if event is None:\n",
    "            raise ValueError(\"event must not be None.\")\n",
    "        if predictions.dim() != event.dim():\n",
    "            raise ValueError(\n",
    "                \"Rank of predictions ({}) must equal rank of event ({})\".format(predictions.dim(), event.dim())\n",
    "            )\n",
    "        if event.size()[1] != 1:\n",
    "            raise ValueError(\"last dimension event ({}) must be 1.\".format(event.size()[1]))\n",
    "        if riskset is None:\n",
    "            raise ValueError(\"riskset must not be None.\")\n",
    "\n",
    "        event = event.type_as(predictions)\n",
    "        riskset = riskset.type_as(predictions)\n",
    "        predictions = safe_normalize(predictions)\n",
    "\n",
    "        # move batch dimension to the end so predictions get broadcast\n",
    "        # row-wise when multiplying by riskset\n",
    "        pred_t = predictions.t()\n",
    "\n",
    "        # compute log of sum over risk set for each row\n",
    "        rr = logsumexp_masked(pred_t, riskset, dim=1, keepdim=True)\n",
    "        assert rr.size() == predictions.size()\n",
    "\n",
    "        losses = event * (rr - predictions)\n",
    "        loss = torch.mean(losses)\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train_model(model, dataloader, criterion, optimizer, writer, device, num_epochs=10):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        epoch_loss = 0\n",
    "        for batch in dataloader:\n",
    "            volumes, tabular, target = batch\n",
    "            volumes, tabular , target = volumes.to(device), tabular.to(device), target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(volumes, tabular)['logits'].squeeze()\n",
    "            loss = criterion(predictions, target.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {epoch_loss:.4f}\")\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:05<00:49,  5.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.0489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:10<00:41,  5.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.7315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:15<00:35,  5.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.6831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:20<00:30,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.6214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:25<00:25,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.6155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:30<00:20,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.5963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:35<00:14,  4.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 0.5829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:40<00:09,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 0.5415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:45<00:04,  4.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 0.5188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:49<00:00,  4.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.4992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DAFT(\n",
       "  (conv1): ConvBnReLU(\n",
       "    (conv): Conv3d(1, 4, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (bn): BatchNorm3d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (pool1): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (block1): ResBlock(\n",
       "    (conv1): Conv3d(4, 4, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (bn1): BatchNorm3d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv3d(4, 4, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (bn2): BatchNorm3d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (block2): ResBlock(\n",
       "    (conv1): Conv3d(4, 8, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "    (bn1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv3d(8, 8, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (bn2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (downsample): Sequential(\n",
       "      (0): Conv3d(4, 8, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "      (1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (block3): ResBlock(\n",
       "    (conv1): Conv3d(8, 16, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "    (bn1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (bn2): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (downsample): Sequential(\n",
       "      (0): Conv3d(8, 16, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "      (1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (blockX): DAFTBlock(\n",
       "    (conv1): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "    (bn1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (bn2): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (global_pool): AdaptiveAvgPool3d(output_size=1)\n",
       "    (downsample): Sequential(\n",
       "      (0): Conv3d(16, 32, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "      (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (aux): Sequential(\n",
       "      (aux_base): Linear(in_features=22, out_features=7, bias=False)\n",
       "      (aux_relu): ReLU()\n",
       "      (aux_out): Linear(in_features=7, out_features=32, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (global_pool): AdaptiveAvgPool3d(output_size=1)\n",
       "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DAFT(1, 1)\n",
    "criterion = MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "train_model(model, train_loader, criterion, optimizer, None, None, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_riskset(targets: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create a risk set matrix for survival analysis.\n",
    "    \n",
    "    Args:\n",
    "        targets (torch.Tensor): 1D tensor of target times (e.g., survival times).\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Boolean risk set matrix of shape (N, N), where N is the number of individuals.\n",
    "    \"\"\"\n",
    "    # Expand the tensor to create a pairwise comparison\n",
    "    targets_i = targets.unsqueeze(1)  # Shape: (1, N)\n",
    "    targets_j = targets.unsqueeze(0)  # Shape: (N, 1)\n",
    "    \n",
    "    # Compare: y_j >= y_i\n",
    "    riskset = targets_j >= targets_i  # Shape: (N, N)\n",
    "    \n",
    "    return riskset\n",
    "\n",
    "riskset = create_riskset(target.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(0, 2, target.squeeze().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train_model(model, dataloader, criterion, optimizer, writer, device, num_epochs=10):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        epoch_loss = 0\n",
    "        for batch in dataloader:\n",
    "            volumes, tabular, target = batch\n",
    "            volumes, tabular , target = volumes.to(device), tabular.to(device), target.to(device)\n",
    "            riskset = create_riskset(target.squeeze())\n",
    "            event = np.random.randint(0, 2, target.squeeze().shape)\n",
    "            event = torch.from_numpy(event).float().unsqueeze(1)\n",
    "\n",
    "            \n",
    "            predictions = model(volumes, tabular)['logits']\n",
    "            loss = criterion(predictions, event, riskset)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {epoch_loss:.4f}\")\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:05<01:43,  5.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 8.9117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:10<01:33,  5.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 9.4140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:15<01:29,  5.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 7.6352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:21<01:28,  5.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 9.0520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [00:26<01:20,  5.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 7.3529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [00:31<01:13,  5.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 8.2386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [00:36<01:07,  5.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 8.5292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [00:41<01:01,  5.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 8.6877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [00:46<00:55,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 9.0228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [00:51<00:50,  5.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 8.1613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [00:56<00:45,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Loss: 8.6922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [01:01<00:40,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Loss: 8.0032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [01:06<00:35,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Loss: 8.2580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [01:12<00:30,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Loss: 7.8308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [01:17<00:25,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Loss: 8.0771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [01:22<00:20,  5.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Loss: 8.4761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [01:28<00:15,  5.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Loss: 7.9376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [01:33<00:10,  5.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Loss: 7.0568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [01:38<00:05,  5.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Loss: 7.6958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:43<00:00,  5.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Loss: 9.1093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DAFT(\n",
       "  (conv1): ConvBnReLU(\n",
       "    (conv): Conv3d(1, 4, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (bn): BatchNorm3d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (pool1): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (block1): ResBlock(\n",
       "    (conv1): Conv3d(4, 4, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (bn1): BatchNorm3d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv3d(4, 4, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (bn2): BatchNorm3d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (block2): ResBlock(\n",
       "    (conv1): Conv3d(4, 8, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "    (bn1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv3d(8, 8, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (bn2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (downsample): Sequential(\n",
       "      (0): Conv3d(4, 8, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "      (1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (block3): ResBlock(\n",
       "    (conv1): Conv3d(8, 16, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "    (bn1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (bn2): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (downsample): Sequential(\n",
       "      (0): Conv3d(8, 16, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "      (1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (blockX): DAFTBlock(\n",
       "    (conv1): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "    (bn1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (bn2): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (global_pool): AdaptiveAvgPool3d(output_size=1)\n",
       "    (downsample): Sequential(\n",
       "      (0): Conv3d(16, 32, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "      (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (aux): Sequential(\n",
       "      (aux_base): Linear(in_features=22, out_features=7, bias=False)\n",
       "      (aux_relu): ReLU()\n",
       "      (aux_out): Linear(in_features=7, out_features=32, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (global_pool): AdaptiveAvgPool3d(output_size=1)\n",
       "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DAFT(1, 1)\n",
    "criterion = CoxphLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "train_model(model, train_loader, criterion, optimizer, None, None, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CensoredMSELoss(nn.Module):\n",
    "    def forward(self, predictions, events, times):\n",
    "        \"\"\"\n",
    "        predictions: Predicted survival times (N,)\n",
    "        events: Binary event indicators (N,)\n",
    "        times: True survival times (N,)\n",
    "        \"\"\"\n",
    "        uncensored_loss = torch.mean(\n",
    "            (predictions[events == 1] - times[events == 1]) ** 2\n",
    "        )\n",
    "        censored_loss = torch.mean(\n",
    "            torch.relu(predictions[events == 0] - times[events == 0]) ** 2\n",
    "        )\n",
    "        return uncensored_loss + 0.1 * censored_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train_model(model, dataloader, criterion, optimizer, writer, device, num_epochs=10):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        epoch_loss = 0\n",
    "        for batch in dataloader:\n",
    "            volumes, tabular, target = batch\n",
    "            volumes, tabular , target = volumes.to(device), tabular.to(device), target.to(device)\n",
    "            event = np.random.randint(0, 2, target.squeeze().shape)\n",
    "            event = torch.from_numpy(event).float()\n",
    "\n",
    "            \n",
    "            predictions = model(volumes, tabular)['logits'].squeeze()\n",
    "            loss = criterion(predictions, event, target.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {epoch_loss:.4f}\")\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:05<01:53,  5.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.6761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:11<01:39,  5.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 2.7219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:16<01:30,  5.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 1.9999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:21<01:23,  5.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 1.3190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [00:26<01:17,  5.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.9059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [00:31<01:11,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.7609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [00:36<01:05,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 0.8596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [00:41<00:59,  4.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 0.8409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [00:46<00:55,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 0.7634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [00:51<00:50,  5.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 1.0022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [00:56<00:45,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Loss: 0.8187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [01:01<00:39,  4.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Loss: 0.7482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [01:06<00:34,  4.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Loss: 0.7180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [01:11<00:30,  5.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Loss: 0.7419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [01:16<00:25,  5.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Loss: 0.8041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [01:21<00:20,  5.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Loss: 0.6695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [01:26<00:15,  5.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Loss: 0.8118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [01:31<00:10,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Loss: 0.8238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [01:36<00:05,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Loss: 0.8511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:41<00:00,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Loss: 0.6884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DAFT(\n",
       "  (conv1): ConvBnReLU(\n",
       "    (conv): Conv3d(1, 4, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (bn): BatchNorm3d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (pool1): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (block1): ResBlock(\n",
       "    (conv1): Conv3d(4, 4, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (bn1): BatchNorm3d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv3d(4, 4, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (bn2): BatchNorm3d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (block2): ResBlock(\n",
       "    (conv1): Conv3d(4, 8, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "    (bn1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv3d(8, 8, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (bn2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (downsample): Sequential(\n",
       "      (0): Conv3d(4, 8, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "      (1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (block3): ResBlock(\n",
       "    (conv1): Conv3d(8, 16, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "    (bn1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (bn2): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (downsample): Sequential(\n",
       "      (0): Conv3d(8, 16, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "      (1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (blockX): DAFTBlock(\n",
       "    (conv1): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "    (bn1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (bn2): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (global_pool): AdaptiveAvgPool3d(output_size=1)\n",
       "    (downsample): Sequential(\n",
       "      (0): Conv3d(16, 32, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "      (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (aux): Sequential(\n",
       "      (aux_base): Linear(in_features=22, out_features=7, bias=False)\n",
       "      (aux_relu): ReLU()\n",
       "      (aux_out): Linear(in_features=7, out_features=32, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (global_pool): AdaptiveAvgPool3d(output_size=1)\n",
       "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DAFT(1, 1)\n",
    "criterion = CensoredMSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "train_model(model, train_loader, criterion, optimizer, None, None, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
